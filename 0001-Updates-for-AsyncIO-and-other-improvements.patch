From e2231652bef0efd0b9fd66b526ec5f3d51afbdca Mon Sep 17 00:00:00 2001
From: Steve Bang <sbang@confluent.io>
Date: Fri, 26 Sep 2025 13:27:22 -0700
Subject: [PATCH] Updates for AsyncIO and other improvements

---
 CHANGELOG.md                | 31 ++++++++++++
 DEVELOPER.md                | 94 ++++++++++++++++++++++++++++++++++---
 README.md                   | 90 +++++++++++++++++++++++++----------
 examples/README.md          | 93 +++++++++++++++++++++++++++++++++---
 examples/asyncio_example.py | 56 ++++++++++++++++++----
 5 files changed, 317 insertions(+), 47 deletions(-)

diff --git a/CHANGELOG.md b/CHANGELOG.md
index 9b80c87..f974d67 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,5 +1,36 @@
 # Confluent's Python client for Apache Kafka
 
+## Unreleased
+
+### Added
+
+- AsyncIO Producer (experimental): Introduces `confluent_kafka.aio.AIOProducer` for
+  asynchronous message production in asyncio applications. This API offloads
+  blocking librdkafka calls to a thread pool and schedules common callbacks
+  (`error_cb`, `throttle_cb`, `stats_cb`, `oauth_cb`, `logger`) onto the event
+  loop for safe usage inside async frameworks.
+
+### Features
+
+- Batched async produce: `await aio.AIOProducer(...).produce(topic, value=...)`
+  buffers messages and flushes when the buffer threshold or timeout is reached.
+- Async lifecycle: `await producer.flush()`, `await producer.purge()`, and
+  transactional operations (`init_transactions`, `begin_transaction`,
+  `commit_transaction`, `abort_transaction`).
+
+### Limitations
+
+- Per-message headers are not supported in the current batched async produce
+  path. If headers are required, use the synchronous `Producer.produce(...)` or
+  offload a sync produce call to a thread executor within your async app.
+
+### Guidance
+
+- Use the AsyncIO Producer inside async apps/servers (FastAPI/Starlette, aiohttp,
+  asyncio tasks) to avoid blocking the event loop.
+- For batch jobs, scripts, or highest-throughput pipelines without an event
+  loop, the synchronous `Producer` remains recommended.
+
 ## v2.11.1
 
 v2.11.1 is a maintenance release with the following fixes:
diff --git a/DEVELOPER.md b/DEVELOPER.md
index 8f24186..59e9890 100644
--- a/DEVELOPER.md
+++ b/DEVELOPER.md
@@ -10,15 +10,19 @@ This document provides information useful to developers working on confluent-kaf
 - Git
 - librdkafka (for Kafka functionality)
 
-### Setup Steps
+### Quick start (editable install)
+
+<!-- markdownlint-disable MD029 -->
 
 1. **Fork and Clone**
+
    ```bash
    git clone https://github.com/your-username/confluent-kafka-python.git
    cd confluent-kafka-python
    ```
 
 2. **Create Virtual Environment**
+
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
@@ -27,7 +31,8 @@ This document provides information useful to developers working on confluent-kaf
 **Note**: On Windows the variables for Visual Studio are named INCLUDE and LIB
 
 3. **Install librdkafka** (if not already installed)
-See the main README.md for platform-specific installation instructions
+
+See the main README.md for platform-specific installation instructions.
 
 If librdkafka is installed in a non-standard location provide the include and library directories with:
 
@@ -35,22 +40,35 @@ If librdkafka is installed in a non-standard location provide the include and li
 C_INCLUDE_PATH=/path/to/include LIBRARY_PATH=/path/to/lib python -m build
 ```
 
-4. **Install confluent-kafka-python with optional dependencies**
+4. **Install confluent-kafka-python (editable) with dev/test/docs extras**
+
    ```bash
    pip3 install -e .[dev,tests,docs]
    ```
 
-   This will also build the wheel be default. Alternatively you can build the bundle independently with:
+   Alternatively you can build the bundle independently with:
 
    ```bash
    python3 -m build
    ```
 
 5. **Verify Setup**
+
    ```bash
    python3 -c "import confluent_kafka; print('Setup successful!')"
    ```
 
+<!-- markdownlint-enable MD029 -->
+
+## Project layout
+
+- `src/confluent_kafka/` — core sync client APIs
+- `src/confluent_kafka/aio/` — AsyncIO Producer/Consumer (first-class asyncio, not generated)
+- `src/confluent_kafka/schema_registry/` — Schema Registry clients and serdes
+- `tests/` — unit and integration tests (including async producer tests)
+- `examples/` — runnable samples (includes asyncio example)
+- `tools/unasync.py` — SR-only sync code generation from async sources
+
 ## Generate Documentation
 
 Install docs dependencies:
@@ -75,7 +93,7 @@ python3 setup.py build_sphinx
 
 Documentation will be generated in  `build/sphinx/html`.
 
-## Unasync -- maintaining sync versions of async code
+## Unasync — maintaining sync versions of async code (Schema Registry only)
 
 ```bash
 python3 tools/unasync.py
@@ -84,10 +102,72 @@ python3 tools/unasync.py
 python3 tools/unasync.py --check
 ```
 
-If you make any changes to the async code (in `src/confluent_kafka/schema_registry/_async` and `tests/integration/schema_registry/_async`), you **must** run this script to generate the sync counter parts (in `src/confluent_kafka/schema_registry/_sync` and `tests/integration/schema_registry/_sync`). Otherwise, this script will be run in CI with the --check flag and fail the build.
+If you make any changes to the async code (in `src/confluent_kafka/schema_registry/_async` and `tests/integration/schema_registry/_async`), you **must** run this script to generate the sync counterparts (in `src/confluent_kafka/schema_registry/_sync` and `tests/integration/schema_registry/_sync`). Otherwise, this script will be run in CI with the `--check` flag and fail the build.
 
+Note: The AsyncIO Producer/Consumer under `src/confluent_kafka/aio/` are first-class asyncio implementations and are not generated using `unasync`.
 
-## Tests
+## AsyncIO Producer development (AIOProducer)
 
+Source:
+
+- `src/confluent_kafka/aio/producer/_AIOProducer.py` (public async API)
+- Internal modules in `src/confluent_kafka/aio/producer/` and helpers in `src/confluent_kafka/aio/_common.py`
+
+For a complete usage example, see [`examples/asyncio_example.py`](examples/asyncio_example.py).
+
+Design guidelines:
+
+- Offload blocking librdkafka calls using `_common.async_call` with a `ThreadPoolExecutor`.
+- Wrap common callbacks (`error_cb`, `throttle_cb`, `stats_cb`, `oauth_cb`, `logger`) onto the event loop using `_common.wrap_common_callbacks`.
+- Batched async produce flushes on batch size or timeout; per-message headers are not supported in batched async produce.
+- Ensure `await producer.flush()` and `await producer.close()` are called in shutdown paths to stop background tasks.
+
+Event loop safety:
+
+- Do not block the event loop. Any new blocking operations must be routed using `_common.async_call`.
+- When exposing callback entry points, use `asyncio.run_coroutine_threadsafe` to re-enter the loop if invoked from non-loop threads.
+
+## Running tests
+
+Unit tests:
+
+```bash
+pytest -q
+```
+
+Run async producer tests only:
+
+```bash
+pytest -q tests/test_AIOProducer.py
+pytest -q -k AIOProducer
+```
+
+Integration tests (may require local/CI Kafka cluster; see tests/README.md):
+
+```bash
+pytest -q tests/integration
+```
+
+## Tests
 
 See [tests/README.md](tests/README.md) for instructions on how to run tests.
+
+## Linting & formatting (suggested)
+
+- Python: `black .` and `flake8` (or `ruff`) per project configuration
+- Markdown: `markdownlint '**/*.md'`
+
+## Documentation build
+
+See “Generate Documentation” above; ensure examples and code blocks compile where applicable.
+
+## Contributing
+
+- Use topic branches (e.g., `feature/asyncio-improvements`).
+- Keep edits focused; include tests where possible.
+- Follow existing code style and add type hints to public APIs where feasible.
+
+## Troubleshooting
+
+- Build errors related to librdkafka: ensure headers and libraries are discoverable; see “Install librdkafka” above for `C_INCLUDE_PATH` and `LIBRARY_PATH`.
+- Async tests hanging: check event loop usage and that `await producer.close()` is called to stop background tasks.
diff --git a/README.md b/README.md
index 1d70629..f136a28 100644
--- a/README.md
+++ b/README.md
@@ -1,14 +1,14 @@
+# Confluent's Python Client for Apache Kafka
+=======================================================
+
 > [!WARNING]
 > Due to an error in which we included dependency changes to a recent patch release, Confluent recommends users to **refrain from upgrading to 2.6.2** of Confluent Kafka. Confluent will release a new minor version, 2.7.0, where the dependency changes will be appropriately included. Users who have already upgraded to 2.6.2 and made the required dependency changes are free to remain on that version and are recommended to upgrade to 2.7.0 when that version is available. Upon the release of 2.7.0, the 2.6.2 version will be marked deprecated.
-We apologize for the inconvenience and appreciate the feedback that we have gotten from the community.
+> We apologize for the inconvenience and appreciate the feedback that we have gotten from the community.
 
 [![Try Confluent Cloud - The Data Streaming Platform](https://images.ctfassets.net/8vofjvai1hpv/10bgcSfn5MzmvS4nNqr94J/af43dd2336e3f9e0c0ca4feef4398f6f/confluent-banner-v2.svg)](https://confluent.cloud/signup?utm_source=github&utm_medium=banner&utm_campaign=tm.plg.cflt-oss-repos&utm_term=confluent-kafka-python)
 
-Confluent's Python Client for Apache Kafka<sup>TM</sup>
-=======================================================
-
-**confluent-kafka-python** provides a high-level Producer, Consumer and AdminClient compatible with all
-[Apache Kafka<sup>TM<sup>](http://kafka.apache.org/) brokers >= v0.8, [Confluent Cloud](https://www.confluent.io/confluent-cloud/)
+**confluent-kafka-python** provides a high-level `Producer`, `Consumer` and `AdminClient` compatible with all
+[Apache Kafka™](http://kafka.apache.org/) brokers >= v0.8, [Confluent Cloud](https://www.confluent.io/confluent-cloud/)
 and [Confluent Platform](https://www.confluent.io/product/compare/). The client is:
 
 - **Reliable** - It's a wrapper around [librdkafka](https://github.com/edenhill/librdkafka) (provided automatically via binary wheels) which is widely deployed in a diverse set of production scenarios. It's tested using [the same set of system tests](https://github.com/confluentinc/confluent-kafka-python/tree/master/src/confluent_kafka/kafkatest) as the Java client [and more](https://github.com/confluentinc/confluent-kafka-python/tree/master/tests). It's supported by [Confluent](https://confluent.io).
@@ -23,9 +23,10 @@ pace with core Apache Kafka and components of the [Confluent Platform](https://w
 
 ## Usage
 
-For a step-by-step guide on using the client see [Getting Started with Apache Kafka and Python](https://developer.confluent.io/get-started/python/).
+For a step-by-step guide on using the client, see [Getting Started with Apache Kafka and Python](https://developer.confluent.io/get-started/python/).
+
+Additional examples can be found in the [examples](examples) directory or the [confluentinc/examples](https://github.com/confluentinc/examples/tree/master/clients/cloud/python) GitHub repo, which include demonstrations of:
 
-Aditional examples can be found in the [examples](examples) directory or the [confluentinc/examples](https://github.com/confluentinc/examples/tree/master/clients/cloud/python) github repo, which include demonstration of:
 - Exactly once data processing using the transactional API.
 - Integration with asyncio.
 - (De)serializing Protobuf, JSON, and Avro data with Confluent Schema Registry integration.
@@ -35,7 +36,47 @@ Also refer to the [API documentation](http://docs.confluent.io/current/clients/c
 
 Finally, the [tests](tests) are useful as a reference for example usage.
 
-### Basic Producer Example
+
+### AsyncIO Producer (experimental)
+
+Use the AsyncIO `Producer` inside async applications to avoid blocking the event loop.
+
+```python
+import asyncio
+from confluent_kafka.aio import AIOProducer
+
+async def main():
+    p = AIOProducer({"bootstrap.servers": "mybroker"})
+    try:
+        # produce() returns a Future; first await the coroutine to get the Future,
+        # then await the Future to get the delivered Message.
+        delivery_future = await p.produce("mytopic", value=b"hello")
+        delivered_msg = await delivery_future
+        # Optionally flush any remaining buffered messages before shutdown
+        await p.flush()
+    finally:
+        await p.close()
+
+asyncio.run(main())
+```
+
+Notes:
+
+- Batched async produce buffers messages; delivery callbacks, stats, errors, and logger run on the event loop.
+- Per-message headers are not supported in the batched async path. If headers are required, use the synchronous `Producer.produce(...)` (you can offload to a thread in async apps).
+
+For a more detailed example that includes both an async producer and consumer, see
+[`examples/asyncio_example.py`](examples/asyncio_example.py).
+
+
+#### When to use AsyncIO vs synchronous Producer
+
+- Use AsyncIO `Producer` when your code runs under an event loop (FastAPI/Starlette, aiohttp, asyncio workers) and must not block.
+- Use synchronous `Producer` for scripts, batch jobs, and highest-throughput pipelines where you control threads/processes and can call `poll()`/`flush()` directly.
+- In async servers, prefer AsyncIO `Producer`; if you need headers, call sync `produce()` via `run_in_executor` for that path.
+
+
+### Basic Producer example
 
 ```python
 from confluent_kafka import Producer
@@ -69,7 +110,7 @@ For a discussion on the poll based producer API, refer to the
 blog post.
 
 
-### Basic Consumer Example
+### Basic Consumer example
 
 ```python
 from confluent_kafka import Consumer
@@ -97,7 +138,7 @@ c.close()
 ```
 
 
-### Basic AdminClient Example
+### Basic AdminClient example
 
 Create topics:
 
@@ -123,9 +164,9 @@ for topic, f in fs.items():
 ```
 
 
-## Thread Safety
+## Thread safety
 
-The `Producer`, `Consumer` and `AdminClient` are all thread safe.
+The `Producer`, `Consumer`, and `AdminClient` are all thread safe.
 
 
 ## Install
@@ -136,7 +177,7 @@ The `Producer`, `Consumer` and `AdminClient` are all thread safe.
 pip install confluent-kafka
 ```
 
-**NOTE:** The pre-built Linux wheels do NOT contain SASL Kerberos/GSSAPI support.
+**NOTE**: The pre-built Linux wheels do NOT contain SASL Kerberos/GSSAPI support.
           If you need SASL Kerberos/GSSAPI support you must install librdkafka and
           its dependencies using the repositories below and then build
           confluent-kafka using the instructions in the
@@ -160,7 +201,7 @@ To use Schema Registry with the Protobuf serializer/deserializer:
 pip install "confluent-kafka[protobuf,schemaregistry]"
 ```
 
-When using Data Contract rules (including CSFLE) add the `rules`extra, e.g.:
+When using Data Contract rules (including CSFLE) add the `rules` extra, e.g.:
 
 ```bash
 pip install "confluent-kafka[avro,schemaregistry,rules]"
@@ -171,7 +212,7 @@ pip install "confluent-kafka[avro,schemaregistry,rules]"
 For source install, see the *Install from source* section in [INSTALL.md](INSTALL.md).
 
 
-## Broker Compatibility
+## Broker compatibility
 
 The Python client (as well as the underlying C library librdkafka) supports
 all broker versions &gt;= 0.8.
@@ -180,8 +221,8 @@ is not safe for a client to assume what protocol version is actually supported
 by the broker, thus you will need to hint the Python client what protocol
 version it may use. This is done through two configuration settings:
 
- * `broker.version.fallback=YOUR_BROKER_VERSION` (default 0.9.0.1)
- * `api.version.request=true|false` (default true)
+- `broker.version.fallback=YOUR_BROKER_VERSION` (default 0.9.0.1)
+- `api.version.request=true|false` (default true)
 
 When using a Kafka 0.10 broker or later you don't need to do anything
 (`api.version.request=true` is the default).
@@ -190,13 +231,12 @@ If you use Kafka broker 0.9 or 0.8 you must set
 `broker.version.fallback` to your broker version,
 e.g `broker.version.fallback=0.9.0.1`.
 
-More info here:
-https://github.com/edenhill/librdkafka/wiki/Broker-version-compatibility
+More info: [Broker version compatibility wiki](https://github.com/edenhill/librdkafka/wiki/Broker-version-compatibility)
 
 
 ## SSL certificates
 
-If you're connecting to a Kafka cluster through SSL you will need to configure
+If you're connecting to a Kafka cluster through SSL, you will need to configure
 the client with `'security.protocol': 'SSL'` (or `'SASL_SSL'` if SASL
 authentication is used).
 
@@ -206,7 +246,7 @@ or `/usr/lib/ssl/cacert.pem`. CA certificates are typically provided by the
 Linux distribution's `ca-certificates` package which needs to be installed
 through `apt`, `yum`, et.al.
 
-If your system stores CA certificates in another location you will need to
+If your system stores CA certificates in another location, you will need to
 configure the client with `'ssl.ca.location': '/path/to/cacert.pem'`.
 
 Alternatively, the CA certificates can be provided by the [certifi](https://pypi.org/project/certifi/)
@@ -223,11 +263,11 @@ by confluent-kafka-python. confluent-kafka-python has no affiliation with and is
 The Apache Software Foundation.
 
 
-## Developer Notes
+## Developer notes
 
-Instructions on building and testing confluent-kafka-python can be found [here](DEVELOPER.md).
+Instructions on building and testing confluent-kafka-python can be found in [DEVELOPER.md](DEVELOPER.md).
 
 
 ## Confluent Cloud
 
-For a step-by-step guide on using the Python client with Confluent Cloud see [Getting Started with Apache Kafka and Python](https://developer.confluent.io/get-started/python/) on [Confluent Developer](https://developer.confluent.io/). 
+For a step-by-step guide on using the Python client with Confluent Cloud, see [Getting Started with Apache Kafka and Python](https://developer.confluent.io/get-started/python/) on [Confluent Developer](https://developer.confluent.io/). 
diff --git a/examples/README.md b/examples/README.md
index 1df0bc0..262d564 100644
--- a/examples/README.md
+++ b/examples/README.md
@@ -1,24 +1,103 @@
+# Confluent Kafka Python Examples
+
 The scripts in this directory provide various examples of using Confluent's Python client for Kafka:
 
-* [adminapi.py](adminapi.py): Various AdminClient operations.
-* [asyncio_example.py](asyncio_example.py): AsyncIO webserver with Kafka producer.
-* [consumer.py](consumer.py): Read messages from a Kafka topic.
+## Basic Producer/Consumer Examples
+
 * [producer.py](producer.py): Read lines from stdin and send them to a Kafka topic.
+* [consumer.py](consumer.py): Read messages from a Kafka topic.
+
+## AsyncIO Examples
+
+* [asyncio_example.py](asyncio_example.py): Comprehensive AsyncIO example demonstrating both AIOProducer and AIOConsumer with transactional operations, batched async produce, proper event loop integration, signal handling, and async callback patterns.
+
+## Transactional Examples
+
 * [eos_transactions.py](eos_transactions.py): Transactional producer with exactly once semantics (EOS).
+
+## Schema Registry & Serialization Examples
+
+### Avro Serialization
+
 * [avro_producer.py](avro_producer.py): Produce Avro serialized data using AvroSerializer.
 * [avro_consumer.py](avro_consumer.py): Read Avro serialized data using AvroDeserializer.
+* [avro_producer_encryption.py](avro_producer_encryption.py): Produce Avro data with client-side field level encryption (CSFLE).
+* [avro_consumer_encryption.py](avro_consumer_encryption.py): Consume Avro data with client-side field level encryption (CSFLE).
+
+### JSON Serialization
+
 * [json_producer.py](json_producer.py): Produce JSON serialized data using JSONSerializer.
 * [json_consumer.py](json_consumer.py): Read JSON serialized data using JSONDeserializer.
+* [json_producer_encryption.py](json_producer_encryption.py): Produce JSON data with client-side field level encryption (CSFLE).
+* [json_consumer_encryption.py](json_consumer_encryption.py): Consume JSON data with client-side field level encryption (CSFLE).
+
+### Protobuf Serialization
+
 * [protobuf_producer.py](protobuf_producer.py): Produce Protobuf serialized data using ProtobufSerializer.
 * [protobuf_consumer.py](protobuf_consumer.py): Read Protobuf serialized data using ProtobufDeserializer.
-* [sasl_producer.py](sasl_producer.py):  Demonstrates SASL Authentication.
-* [get_watermark_offsets.py](get_watermark_offsets.py): Consumer method for listing committed offsets and consumer lag for group and topics.
+* [protobuf_producer_encryption.py](protobuf_producer_encryption.py): Produce Protobuf data with client-side field level encryption (CSFLE).
+* [protobuf_consumer_encryption.py](protobuf_consumer_encryption.py): Consume Protobuf data with client-side field level encryption (CSFLE).
+
+## Authentication Examples
+
+* [sasl_producer.py](sasl_producer.py): Demonstrates SASL Authentication.
 * [oauth_producer.py](oauth_producer.py): Demonstrates OAuth Authentication (client credentials).
+* [oauth_oidc_ccloud_producer.py](oauth_oidc_ccloud_producer.py): Demonstrates OAuth OIDC authentication with Confluent Cloud.
+* [oauth_schema_registry.py](oauth_schema_registry.py): Demonstrates OAuth authentication with Schema Registry.
 
-Additional examples for [Confluent Cloud](https://www.confluent.io/confluent-cloud/):
+## Admin API Examples
+
+* [adminapi.py](adminapi.py): Various AdminClient operations (topics, configs, ACLs).
+* [adminapi_logger.py](adminapi_logger.py): AdminClient operations with custom logger configuration.
+* [get_watermark_offsets.py](get_watermark_offsets.py): Consumer method for listing committed offsets and consumer lag for group and topics.
+
+## Confluent Cloud Examples
 
 * [confluent_cloud.py](confluent_cloud.py): Produce messages to Confluent Cloud and then read them back again.
-* [confluentinc/examples](https://github.com/confluentinc/examples/tree/master/clients/cloud/python): Integration with Confluent Cloud and Confluent Cloud Schema Registry
+* [confluentinc/examples](https://github.com/confluentinc/examples/tree/master/clients/cloud/python): Integration with Confluent Cloud and Confluent Cloud Schema Registry.
+
+**Confluent Cloud Resources:**
+
+* [Getting Started with Python](https://developer.confluent.io/get-started/python/) - Step-by-step tutorial.
+* [Confluent Cloud Console](https://confluent.cloud/) - Sign up and manage clusters.
+* [Python Client Configuration for Confluent Cloud](https://docs.confluent.io/cloud/current/client-apps/config-client.html#python-client)
+* [Confluent Developer](https://developer.confluent.io/) - Tutorials, guides, and code examples.
+
+## Running the Examples
+
+Most examples require a running Kafka cluster. You can use:
+
+* Local Kafka installation
+* Docker Compose (see `docker/` subdirectory)
+* [Confluent Cloud](https://confluent.cloud/) - see [Getting Started with Python](https://developer.confluent.io/get-started/python/) guide.
+
+### Basic Usage Pattern
+
+```bash
+# Most examples follow this pattern:
+python3 <example_name>.py <bootstrap_servers> [additional_args]
+
+# For example:
+python3 producer.py localhost:9092
+python3 consumer.py localhost:9092
+python3 asyncio_example.py localhost:9092 my-topic
+```
+
+### Examples with Schema Registry
+
+For Avro, JSON, and Protobuf examples, you'll also need a Schema Registry:
+
+```bash
+python3 avro_producer.py localhost:9092 http://localhost:8081
+```
+
+**Schema Registry Resources:**
+
+* [Schema Registry Overview](https://docs.confluent.io/platform/current/schema-registry/index.html)
+* [Using Schema Registry with Python](https://docs.confluent.io/kafka-clients/python/current/overview.html#schema-registry)
+* [Confluent Cloud Schema Registry](https://docs.confluent.io/cloud/current/sr/index.html)
+
+Check each example's source code for specific command-line arguments and configuration requirements.
 
 ## venv setup
 
diff --git a/examples/asyncio_example.py b/examples/asyncio_example.py
index 52b26b9..8ab835c 100644
--- a/examples/asyncio_example.py
+++ b/examples/asyncio_example.py
@@ -22,12 +22,22 @@ import random
 import logging
 import signal
 
+# This example demonstrates comprehensive AsyncIO usage patterns with Kafka:
+# - Event loop safe callbacks that don't block the loop
+# - Batched async produce with transaction handling
+# - Proper async consumer with partition management
+# - Graceful shutdown with signal handling
+# - Thread pool integration for blocking operations
+
 logging.basicConfig()
 logger = logging.getLogger(__name__)
 logger.setLevel(logging.DEBUG)
 running = True
 
 
+# AsyncIO Pattern: Event loop safe callbacks
+# These callbacks are automatically scheduled onto the event loop by AIOProducer/AIOConsumer
+# ensuring they don't block the loop and can safely interact with other async operations
 async def error_cb(err):
     logger.error(f'Kafka error: {err}')
 
@@ -57,11 +67,15 @@ def configure_common(conf):
 
 async def run_producer():
     topic = sys.argv[2]
+    # AsyncIO Pattern: Non-blocking producer with thread pool
+    # max_workers=5 creates a ThreadPoolExecutor for offloading blocking librdkafka calls
     producer = AIOProducer(configure_common(
         {
             'transactional.id': 'producer1'
         }), max_workers=5)
 
+    # AsyncIO Pattern: Async transaction lifecycle
+    # All transaction operations are awaitable and won't block the event loop
     await producer.init_transactions()
     # TODO: handle exceptions with transactional API
     transaction_active = False
@@ -70,11 +84,15 @@ async def run_producer():
             await producer.begin_transaction()
             transaction_active = True
 
+            # AsyncIO Pattern: Batched async produce with concurrent futures
+            # Creates 100 concurrent produce operations, each returning a Future
+            # that resolves when the message is delivered or fails
             produce_futures = [asyncio.create_task(
                 producer.produce(topic=topic,
                                  key=f'testkey{i}',
                                  value=f'testvalue{i}'))
                                for i in range(100)]
+            # Wait for all produce operations to complete concurrently
             results = await asyncio.gather(*produce_futures)
 
             for msg in results:
@@ -83,35 +101,43 @@ async def run_producer():
                                                        msg.partition(),
                                                        msg.offset()))
 
+            # AsyncIO Pattern: Non-blocking transaction commit
             await producer.commit_transaction()
             transaction_active = False
+            # Use asyncio.sleep() instead of time.sleep() to yield control to event loop
             await asyncio.sleep(1)
     except Exception as e:
         logger.error(e)
     finally:
+        # AsyncIO Pattern: Proper async cleanup
+        # Always clean up resources asynchronously to avoid blocking the event loop
         if transaction_active:
             await producer.abort_transaction()
-        await producer.stop()
+        await producer.stop()  # Stops background tasks and closes connections
         logger.info('Closed producer')
 
 
 async def run_consumer():
     topic = sys.argv[2]
     group_id = f'{topic}_{random.randint(1, 1000)}'
+    # AsyncIO Pattern: Non-blocking consumer with manual offset management
+    # Callbacks will be scheduled on the event loop automatically
     consumer = AIOConsumer(configure_common(
         {
             'group.id': group_id,
             'auto.offset.reset': 'latest',
-            'enable.auto.commit': 'false',
-            'enable.auto.offset.store': 'false',
+            'enable.auto.commit': 'false',  # Manual commit for precise control
+            'enable.auto.offset.store': 'false',  # Manual offset storage
             'partition.assignment.strategy': 'cooperative-sticky',
         }))
 
+    # AsyncIO Pattern: Async rebalance callbacks
+    # These callbacks can perform async operations safely within the event loop
     async def on_assign(consumer, partitions):
         # Calling incremental_assign is necessary to pause the assigned partitions
         # otherwise it'll be done by the consumer after callback termination.
         await consumer.incremental_assign(partitions)
-        await consumer.pause(partitions)
+        await consumer.pause(partitions)  # Demonstrates async partition control
         logger.debug(f'on_assign {partitions}')
         # Resume the partitions as it's just a pause example
         await consumer.resume(partitions)
@@ -119,7 +145,8 @@ async def run_consumer():
     async def on_revoke(consumer, partitions):
         logger.debug(f'before on_revoke {partitions}', )
         try:
-            await consumer.commit()
+            # AsyncIO Pattern: Non-blocking commit during rebalance
+            await consumer.commit()  # Ensure offsets are committed before losing partitions
         except Exception as e:
             logger.info(f'Error during commit: {e}')
         logger.debug(f'after on_revoke {partitions}')
@@ -137,14 +164,18 @@ async def run_consumer():
                                  on_lost=on_lost)
         i = 0
         while running:
+            # AsyncIO Pattern: Non-blocking message polling
+            # poll() returns a coroutine that yields control back to the event loop
             message = await consumer.poll(1.0)
             if message is None:
                 continue
 
             if i % 100 == 0:
+                # AsyncIO Pattern: Async metadata operations
+                # Both assignment() and position() are async and won't block the loop
                 position = await consumer.position(await consumer.assignment())
                 logger.info(f'Current position: {position}')
-                await consumer.commit()
+                await consumer.commit()  # Async commit of stored offsets
                 logger.info('Stored offsets were committed')
 
             err = message.error()
@@ -152,14 +183,19 @@ async def run_consumer():
                 logger.error(f'Error: {err}')
             else:
                 logger.info(f'Consumed: {message.value()}')
+                # AsyncIO Pattern: Async offset storage
                 await consumer.store_offsets(message=message)
                 i += 1
     finally:
-        await consumer.unsubscribe()
-        await consumer.close()
+        # AsyncIO Pattern: Proper async consumer cleanup
+        # Always unsubscribe and close asynchronously
+        await consumer.unsubscribe()  # Leave consumer group gracefully
+        await consumer.close()  # Close connections and stop background tasks
         logger.info('Closed consumer')
 
 
+# AsyncIO Pattern: Signal handling for graceful shutdown
+# Sets a flag that async tasks check to terminate cleanly
 def signal_handler(*_):
     global running
     logger.info('Signal received, shutting down...')
@@ -167,11 +203,15 @@ def signal_handler(*_):
 
 
 async def main():
+    # AsyncIO Pattern: Signal handling setup
     signal.signal(signal.SIGINT, signal_handler)
     signal.signal(signal.SIGTERM, signal_handler)
 
+    # AsyncIO Pattern: Concurrent task execution
+    # Both producer and consumer run concurrently in the same event loop
     producer_task = asyncio.create_task(run_producer())
     consumer_task = asyncio.create_task(run_consumer())
+    # Wait for both tasks to complete (or be cancelled by signal)
     await asyncio.gather(producer_task, consumer_task)
 
 try:
-- 
2.50.1 (Apple Git-155)

